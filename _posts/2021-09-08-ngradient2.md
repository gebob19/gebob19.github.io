---
title:  "Diagonal and Kronecker-Factored Approximate Curvature (K-FAC) Approximations for Natural Gradient Descent"
description: "This blog post/tutorial dives deep into the theory and JAX code for understanding how to use kronecker factorization to efficiently compute the natural gradient. This post follows the first post introducing the natural gradient."
layout: post
categories: journal
tags: [documentation,sample]
image: italy2.jpeg
mathjax: true
published: true
---

## Problems with MVPs in Natural Gradients:

In the last post we introduced the theory behind Natural Gradient descent and how to implement them using Matrix Vector Products (MVPs) (i.e, Hessian Free Optimization). However, iterating through the Conjugate Gradient algorithm $n$ times at each gradient step can be quite expensive. To review, below is a psuedocode of the MVP Natural Gradient algorithm,

Loop: 
1. Compute gradients $$\nabla L(w)$$ 
2. Define the MVP: `lambda v:` $$\text{F}_w v$$ 
3. Compute $$\text{F}_w^{-1} \nabla L(w)$$ using the conjugate gradient method and the MVP (**expensive**)
4. Take a step: $$w^{(k+1)} = w^{(k)} - \lambda^{-1} \text{F}_w^{-1}\nabla L(w^{(k)})$$  

Psuedocode: 

    logits = model(x)
    loss_function(logits, targets).backward()
    MVP = lambda v: ...
    F_inv_grads = conjugate_gradients(gradients, MVP) # expensive

    model.params = model.params - alpha * F_inv_grads

Explicitly, Step 3 can be very expensive. Ideally, it would be nice if we could derive a **parametric** approximation which we could use and only update every n-iterations. For example an ideal parametric loop would be as follows: 

Loop: 
1. Compute gradients $$\nabla L(w)$$ 
2. if iteration % n: update parametric approximation (**expensive**)
3. Compute $$\text{F}_w^{-1} \nabla L(w)$$ using parametric approximation (**cheap**)
4. Take a step: $$w^{(k+1)} = w^{(k)} - \lambda^{-1} \text{F}_w^{-1}\nabla L(w^{(k)})$$  

*Note:* MVPs aren't a parametric approximation since we don't store the parameters explicitly (i.e, we compute them only when called with a vector)

In this derivation we are swapping an expensive operation at *each step* in MVPs (i.e, n Conjugate Gradient steps) with an expensive operation *every n-steps* in the loop. Doing so can make our training loop much more efficient. 

One algorithm which does exactly this, which we will explore in this blog post, is Kronecker-factored Approximate Curvature (K-FAC). 

***

Most of this post is created in reference to Roger Grosse's 'Neural Net Training Dynamics' course. The course can be found [here](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/) and I would highly recommend checking it out. Though there are no lecture recordings, the course notes are in a league of their own.

***

## Diagonal Approximations 

Before we talk about K-FAC, it makes sense to talk about a simpler parametric approximation: the Diagonal approximation.

First, recall the definition of the fisher information matrix,

$$\text{F} = \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \nabla_\theta \log p(x \vert \theta) \, \nabla_\theta \log p(x \vert \theta)^{\text{T}} \right]$$

Recall, a straight forward and naive way to compute $F$ would be: 

1. Compute the gradients $$\nabla L(w^{(k)})$$ using autodiff and flatten/reshape them into a $n \times 1$ matrix
2. Compute the fisher matrix $$F_w = \nabla L \nabla L^T$$ as a $n \times n$ matrix

Psuedocode of the algorithm is below, 

    logits = model(x) # model forward pass

    sampled_target = logits.sample() # fisher sample y ~ p(x | theta)
    loss = loss_function(logits, sampled_target) # \nabla p(y | theta)
    loss.backward() # compute gradients on sampled loss 

    fgrads = [grad.flatten() for grad in model] 
    F = fgrads @ fgrads.T

However, we know with neural networks $n$ is usually in the millions so this isn't possible. 

To make this feasible, one approximation we could make is a **diagonal approximation**. This would only requiring a $n \times 1$ vector and since the inverse of a diagonal matrix is the inverse of its diagonal values, computing $F^{-1}$ would be easy. 

*Note:* For any vector $X$, $diagonal(X X^T) = X^2 * I$, i.e, the diagonal of a $n \times 1$ matrix dot producted with the same matrix tranposed is just going to be the $n \times 1$ matrix values squared and broadcasted along the diagonals of a $n \times n$ identity matrix. This means we don't need to reshape our gradients to compute $F$ since can just square them as is and then use element-wise multiplication. 

Psuedocode of the algorithm is below: 

    logits = model(x) # model forward pass
    
    # compute F and F_inv 
    sampled_target = logits.sample() # fisher sample y ~ p(x | theta)
    loss = loss_function(logits, sampled_target) # \nabla p(y | theta)
    loss.backward() # compute gradients on sampled loss 

    F = [grad ** 2 for grad in model] # diagonal approximation
    F_inv = 1/F 
    
    # clear gradients 
    zero_grad(model)

    # compute loss on actual targets 
    loss_function(logits, targets).backward()
    # natural gradient step! with element-wise multiplication
    model.params = model.params - alpha * F_inv * gradients


## K-FAC

Although a diagonal approximation usually works pretty well it ignores a lot of curvature information since it assumes all off-diagonal values are zero; we can do better. K-FAC to the rescue! The main assumption K-FAC makes is  that off block-diagonal values are zero, meaning we only compute the second-order information for each layer. Which looks something like this: 

<div align="center" width="500" height="100">
<img src="https://raw.githubusercontent.com/gebob19/gebob19.github.io/source/assets/natural_grad/block-diag.png" alt="block-diagonal" class="center"/>
</div>

Where $\ell_i$ represents the parameters for the $i$-th layer.

First we introduce some K-FAC notation. We can define a linear network layer $\ell$ with pre-activation values $s_\ell$ and activation values $a_\ell$ as: 

$$
s_\ell = \bar{W_\ell} \bar{a}_{\ell-1} \\
a_\ell = \phi_\ell(s_\ell)
$$

We define the linear layer as $\bar{W_\ell} = (W_\ell, b_\ell)$ with W and b are the weight and bias respectively and $\bar{a_{\ell}} = ({a_\ell}^T 1)^T$ which is the activations from the previous layer with a 1 concatenated to add the bias when matrix multiplied. 

*Note:* Its important to note the difference between ($a$, $W$) and ($\bar{a}$, $\bar{W}$) throughout the notes

Then using the chain rule we can derive the gradients 

$$
\mathcal{D} a_\ell = W_\ell^T \mathcal{D} s_{\ell+1}\\
\mathcal{D} s_\ell = \mathcal{D} a_{\ell} \odot \phi'_\ell(s_\ell)\\
\mathcal{D} \bar{W}_\ell = \mathcal{D} s_{\ell} \bar{a}_{\ell-1}^T
$$

With the notation introduced, we can then computing the second order matrix **for the $\ell$-th layer** and with some *approximations* we can derive K-FAC: 

$$\begin{align*}
\hat{F}_{\ell} &= \mathbb{E}[\text{vec}(\mathcal{D} W_\ell) \text{vec}(\mathcal{D} 
W_\ell)^T]\\
&= \mathbb{E}[\text{vec}(\mathcal{D} s_\ell \bar{a}_{\ell-1}^T) \text{vec}(\mathcal{D} s_\ell \bar{a}_{\ell-1}^T)]\\
&\approx \mathbb{E}[\bar{a}_{\ell-1} \bar{a}_{\ell-1}^T] \otimes \mathbb{E}[\mathcal{D}s_\ell \mathcal{D}s_\ell^T]\\
&= \mathbb{E}[A_{\ell-1}] \otimes \mathbb{E}[S_\ell]
\end{align*}$$

Where $\otimes$ is the Kronecker Product. Meaning, to compute the fisher matrix for the $\ell$-th layer: for each forward pass, we need to store 1. the activation values of each layer (A) and 2. the pre-activation gradients of each layer (S).  

We first start with the definition of the fisher matrix and using a few steps find that we can decompose it into the Kronecker Product of two smaller matricies. Note that going from line 2 to 3 is an approximation. 

From here, using some properties of the Kronecker Product we can derive an efficient natural gradient vector product for some vector $v_\ell$ (which will be our gradient for the $l$-th layer): 

$$
\hat{F}^{-1}_{\ell} v_\ell = \text{vec}(S_\ell^{-1}\bar{V}_\ell A_{\ell -1}^{-1})
$$

In short the steps become: 

Loop: 
1. Compute gradients $$\nabla L(w)$$ 
2. if (iteration % n): Update $A_{\ell-1}^{-1}$ and $S_\ell^{-1}$ with data (**expensive**)
3. Compute $$\text{F}_w^{-1} \nabla L(w) = S_\ell^{-1} \nabla L(w)_\ell A_{\ell -1}^{-1}$$ (**cheap**)
4. Take a step: $$w^{(k+1)} = w^{(k)} - \lambda^{-1} \text{F}_w^{-1}\nabla L(w^{(k)})$$  

With Psuedocode as follows: 

    for i in range(420):
        x, targets = batch[i] # get data
        logits = model(x) # model forward pass
    
        if i % inverse_update == 0: # expensive 
            # compute F and F_inv 
            sampled_targets = logits.sample() # fisher sample y ~ p(x | theta)
            loss = loss_function(logits, sampled_targets) # \nabla p(y | theta)
            loss.backward() # compute gradients on sampled loss 

            # compute A and S using backward/forward pytorch hooks 
            A, S = extract_info(model)
            A_inv = inverse(A)
            S_inv = inverse(S)

        zero_grad(model)

        # natural gradient step! 
        loss_function(logits, targets).backward()
        params = params - alpha * (S_inv @ gradients @ A_inv) # cheap

This implementation is very simple but can be updated to use a moving average of A and S values for better performance since they are only expensive to invert and are inexpensive to save. 

