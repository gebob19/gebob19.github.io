<!doctype html>
<html>
  <head>
  <title>
    
      Going with the Flow: An Introduction to Normalizing Flows | Brennan Gebotys
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="https://gebob19.github.io/feed.xml" title="Brennan Gebotys" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Brennan Gebotys | Machine Learning, Statistics, and All Things Cool"/>
  //-->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
  </script>
  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-144191674-1', 'auto');
  ga('send', 'pageview');
</script>

  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Going with the Flow: An Introduction to Normalizing Flows | Brennan Gebotys</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Going with the Flow: An Introduction to Normalizing Flows" />
<meta name="author" content="Brennan Gebotys" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog post/tutorial dives deep into the theory and PyTorch code for Normalizing Flows" />
<meta property="og:description" content="This blog post/tutorial dives deep into the theory and PyTorch code for Normalizing Flows" />
<link rel="canonical" href="https://gebob19.github.io/normalizing-flows/" />
<meta property="og:url" content="https://gebob19.github.io/normalizing-flows/" />
<meta property="og:site_name" content="Brennan Gebotys" />
<meta property="og:image" content="https://gebob19.github.io/wave.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-17T00:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://gebob19.github.io/wave.jpeg" />
<meta property="twitter:title" content="Going with the Flow: An Introduction to Normalizing Flows" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://gebob19.github.io/normalizing-flows/"},"url":"https://gebob19.github.io/normalizing-flows/","author":{"@type":"Person","name":"Brennan Gebotys"},"image":"https://gebob19.github.io/wave.jpeg","description":"This blog post/tutorial dives deep into the theory and PyTorch code for Normalizing Flows","headline":"Going with the Flow: An Introduction to Normalizing Flows","dateModified":"2019-07-17T00:00:00-04:00","datePublished":"2019-07-17T00:00:00-04:00","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="container">
      <header class="header">
  <h3 class="header-title">
    <a href="/">Brennan Gebotys</a>
    <small class="header-subtitle">Machine Learning, Statistics, and All Things Cool</small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/">Home</a>
    
      <a href="/writing.html">Compact Blog</a>
    
      <a href="/about.html">About</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/gebob19" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/brennangebotys" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>

      <div class="content-container">
        <h1>
  Going with the Flow: An Introduction to Normalizing Flows
</h1>

  <img src="/assets/img/wave.jpeg">

<article>
  <p><a href="https://unsplash.com/photos/cYrMQA7a3Wc">Photo Link</a></p>

<p><img src="https://gebob19.github.io/assets/norm_flow/nf.png" alt="alt text" title="Normalizing Flows (from R-NVP Paper)" /></p>

<p>Normalizing Flows (NFs) <a class="citation" href="#rezende2015variational">(Rezende &amp; Mohamed, 2015)</a> learn an <em>invertible</em> mapping \(f: X \rightarrow Z\), where \(X\) is our data distribution and \(Z\) is a chosen latent-distribution.</p>

<p>Normalizing Flows are part of the generative model family, which includes Variational Autoencoders (VAEs) <a class="citation" href="#vaebayes">(Kingma &amp; Welling, 2013)</a>, and Generative Adversarial Networks (GANs) <a class="citation" href="#NIPS2014_5423">(Goodfellow et al., 2014)</a>. Once we learn the mapping \(f\), we generate data by sampling \(z \sim p_Z\) and then applying the inverse transformation, \(f^{-1}(z) = x_{gen}\).</p>

<p><em>Note</em>: \(p_Z(z)\) is the probability density of sampling \(z\) under the distribution \(Z\).</p>

<p>In this blog to understand normalizing flows better, we will cover the algorithm’s theory and implement a flow model in PyTorch. But first, let us flow through the advantages and disadvantages of normalizing flows.</p>

<p><em>Note:</em> If you are not interested in the comparison between generative models you can skip to ‘How Normalizing Flows Work’</p>

<h2 id="why-normalizing-flows">Why Normalizing Flows</h2>

<p>With the amazing results shown by VAEs and GANs, why would you want to use Normalizing flows? We list the advantages below</p>

<p><em>Note</em>: Most advantages are from the GLOW paper <a class="citation" href="#kingma2018glow">(Kingma &amp; Dhariwal, 2018)</a></p>

<ul>
  <li>NFs optimize the exact log-likelihood of the data, log(\(p_X\))
    <ul>
      <li>VAEs optimize the lower bound (ELBO)</li>
      <li>GANs learn to fool a discriminator network</li>
    </ul>
  </li>
  <li>NFs infer exact latent-variable values \(z\), which are useful for downstream tasks
    <ul>
      <li>The VAE infers a distribution over latent-variable values</li>
      <li>GANs do not have a latent-distribution</li>
    </ul>
  </li>
  <li>Potential for memory savings, with NFs gradient computations scaling constant to their depth
    <ul>
      <li>Both VAE’s and GAN’s gradient computations scale linearly to their depth</li>
    </ul>
  </li>
  <li>NFs require only an encoder to be learned
    <ul>
      <li>VAEs require encoder and decoder networks</li>
      <li>GANs require generative and discriminative networks</li>
    </ul>
  </li>
</ul>

<p>But remember what mother says, “There ain’t no such thing as a free lunch”.</p>

<p>Some of the downsides of normalizing flows are as follows,</p>

<ul>
  <li>The requirements of invertibility and efficient Jacobian calculations restrict model architecture
    <ul>
      <li>more on this later…</li>
    </ul>
  </li>
  <li>Less resources/research on NFs compared to other generative models
    <ul>
      <li>The reason for this blog!</li>
    </ul>
  </li>
  <li>NFs generative results are still behind VAEs and GANs</li>
</ul>

<p>Now let us get dirty in some theory!</p>

<h1 id="how-normalizing-flows-work">How Normalizing Flows Work</h1>

<p>In this section, we understand the heart of Normalizing Flows.</p>

<h2 id="probability-distribution-change-of-variables">Probability Distribution Change of Variables</h2>

<p>Consider a random variable \(X \in \mathbb{R}^d\) (our data distribution) and an invertable transformation \(f: \mathbb{R}^d \mapsto \mathbb{R}^d\)</p>

<p>Then there is a random variable \(Z \in \mathbb{R}^d\) which \(f\) maps \(X\) to.</p>

<p>Furthermore,</p>

\[P(X = x) = P(f(X) = f(x)) = P(Z = z)\tag{0}\]

<p>Now consider some interval \(\beta\) over \(X\). Then there exists some interval \(\beta^{\prime}\) over \(Z\) such that,</p>

\[P(X \in \beta) = P(Z \in \beta^{\prime})\tag{1}\]

\[\int_{\beta} p_X dx = \int_{\beta^{\prime}} p_Z dz\tag{2}\]

<p>For the sake of simplicity, we consider a single region.</p>

\[dx \cdot p_X(x) = dz \cdot p_Z(z) \tag{3}\]

\[p_X(x) = \mid\dfrac{dz}{dx}\mid \cdot p_Z(z) \tag{4}\]

<p><em>Note:</em> We apply the absolute value to maintain the equality since by the probability axioms \(p_X\) and \(p_Z\) will always be positive.</p>

\[p_X(x) = \mid\dfrac{df(x)}{dx}\mid \cdot p_Z(f(x)) \tag{5}\]

\[p_X(x) = \mid det(\dfrac{df}{dx}) \mid \cdot p_Z(f(x)) \tag{6}\]

<p><em>Note:</em> We use the determinant to generalize to the multivariate case (\(d &gt; 1\))</p>

\[\log(p_X(x)) = \log(\mid det(\dfrac{df}{dx}) \mid) + \log(p_Z(f(x))) \tag{7}\]

<p>Tada! To model our random variable \(X\), we need to maximize the right-hand side of equation (7).</p>

<p>Breaking the equation down:</p>
<ul>
  <li>\(\log(\mid det(\dfrac{df}{dx}) \mid)\) is the amount of stretch/change \(f\) applies to the probability distribution \(p_X\).
    <ul>
      <li>This term is the log determinant of the Jacobian matrix (\(\dfrac{df}{dx}\)). We refer to the determinant of the Jacobian matrix as the Jacobian.</li>
    </ul>
  </li>
  <li>\(\log(p_Z(f(x)))\) constrains \(f\) to transform \(x\) to the distribution \(p_Z\).</li>
</ul>

<p>Since there are no constraints on \(Z\) we can choose \(p_Z\)! Usually, we choose \(p_Z\) to be gaussian.</p>

<p>Now I know what your thinking, as a reader of this blog you strive for greatness and say,</p>
<blockquote>
  <p>‘Brennan, a single function does not satisfy me. I have a hunger for more.’</p>
</blockquote>

<h2 id="applying-multiple-functions-sequentially">Applying multiple functions sequentially</h2>

<p>Fear not my readers! I will show you how we can sequentially apply multiple functions.</p>

<p>Let \(z_n\) be the result of sequentially applying \(n\) functions to \(x \sim p_X\).</p>

\[z_n = f_n \circ \dots \circ f_1(x) \tag{8}\]

\[f = f_n \circ \dots \circ f_1 \tag{9}\]

<p>Using the handy dandy chain rule, we can modify equation (7) with equation (8) to get equation (10) as follows.</p>

\[\log(p_X(x)) = \log(\mid det(\dfrac{df}{dx}) \mid) + \log(p_Z(f(x))) \tag{7}\]

\[\log(p_X(x)) = \log(\prod_{i=1}^{n} \mid det(\dfrac{dz_i}{dz_{i-1}}) \mid) + \log(p_Z(f(x)))\tag{10}\]

<p>Where \(x \triangleq z_0\) for conciseness.</p>

\[\log(p_X(x)) = \sum_{i=1}^{n} \log(\mid det(\dfrac{dz_i}{dz_{i-1}}) \mid) + \log(p_Z(f(x))) \tag{11}\]

<p>We want the Jacobian term to be easy to compute since we will need to compute it \(n\) times.</p>

<p>To efficiently compute the Jacobian, the functions \(f_i\) (corresponding to \(z_i\)) are chosen to have a lower or upper triangular Jacobian matrix. Since the determinant of a triangular matrix is the product of its diagonal, which is easy to compute.</p>

<p>Now that you understand the general theory of Normalizing flows, lets flow through some PyTorch code.</p>

<h1 id="the-family-of-flows">The Family of Flows</h1>

<p>For this post we will be focusing on, real-valued non-volume preserving flows (R-NVP) <a class="citation" href="#dinh2016density">(Dinh et al., 2016)</a>.</p>

<p>Though there are many other flow functions out and about such as NICE <a class="citation" href="#dinh2014nice">(Dinh et al., 2014)</a>, and GLOW <a class="citation" href="#kingma2018glow">(Kingma &amp; Dhariwal, 2018)</a>. For keeners wanting to learn more, I will show you to the ‘More Resources’ section at the bottom of this post which includes blog posts with more flows which may interest you.</p>

<h1 id="r-nvp-flows">R-NVP Flows</h1>

<p>We consider a single R-NVP function \(f: \mathbb{R}^d \rightarrow \mathbb{R}^d\), with input \(\mathbf{x} \in \mathbb{R}^d\) and output \(\mathbf{z} \in \mathbb{R}^d\).</p>

<p>To quickly recap, in order to optimize our function \(f\) to model our data distribution \(p_X\), we want to know the forward pass \(f\), and the Jacobian \(\mid det(\dfrac{df}{dx}) \mid\).</p>

<p>We then will want to know the inverse of our function \(f^{-1}\) so we can transform a sampled latent-value \(z \sim p_Z\) to our data distribution \(p_X\), generating new samples!</p>

<h2 id="forward-pass">Forward Pass</h2>

\[f(\mathbf{x}) = \mathbf{z}\tag{12}\]

<p>The forward pass is a combination of copying values while stretching and shifting the others. First we choose some arbitrary value \(k\) which satisfies \(0 &lt; k &lt; d\) to split our input.</p>

<p>R-NVPs forward pass is then the following</p>

\[\mathbf{z}_{1:k} = \mathbf{x}_{1:k} \tag{13}\]

\[\mathbf{z}_{k+1:d} = \mathbf{x}_{k+1:d} \odot \exp(\sigma(\mathbf{x}_{1:k})) + \mu(\mathbf{x}_{1:k})\tag{14}\]

<p>Where \(\sigma, \mu: \mathbb{R}^k \rightarrow \mathbb{R}^{d-k}\) and are any arbitrary functions. Hence, we will choose \(\sigma\) and \(\mu\) to both be deep neural networks. Below is PyTorch code of a simple implementation.</p>

<script src="https://gist.github.com/gebob19/1c10929c2b8a7089321e29c4c33dca4a.js"></script>

<h2 id="log-jacobian">Log Jacobian</h2>

<p>The Jacobian matrix \(\dfrac{df}{d\mathbf{x}}\) of this function will be</p>

\[\begin{bmatrix}I_d &amp; 0 \\
\frac{d z_{k+1:d}}{d \mathbf{x}_{1:k}} &amp;   \text{diag}(\exp[\sigma(\mathbf{x}_{1:k})])   \end{bmatrix}  \tag{15}\]

<p>The log determinant of such a Jacobian Matrix will be</p>

\[\log(\det(\dfrac{df}{d\mathbf{x}})) = \log(\prod_{i=1}^{d-k} \mid\exp[\sigma_i(\mathbf{x}_{1:k})]\mid) \tag{16}\]

\[\log(\mid\det(\dfrac{df}{d\mathbf{x}})\mid) = \sum_{i=1}^{d-k} \log(\exp[\sigma_i(\mathbf{x}_{1:k})]) \tag{17}\]

\[\log(\mid\det(\dfrac{df}{d\mathbf{x}})\mid) = \sum_{i=1}^{d-k} \sigma_i(\mathbf{x}_{1:k}) \tag{18}\]

<script src="https://gist.github.com/gebob19/8dc1fe38b73fd350ff63b81f5947111a.js"></script>

<h2 id="inverse">Inverse</h2>

\[f^{-1}(\mathbf{z}) = \mathbf{x}\tag{19}\]

<p>One of the benefits of R-NVPs compared to other flows is the ease of inverting \(f\) into \(f^{-1}\), which we formulate below using the forward pass of equation (14)</p>

\[\mathbf{x}_{1:k} = \mathbf{z}_{1:k} \tag{20}\]

\[\mathbf{x}_{k+1:d} = (\mathbf{z}_{k+1:d} - \mu(\mathbf{x}_{1:k})) \odot \exp(-\sigma(\mathbf{x}_{1:k})) \tag{21}\]

\[\Leftrightarrow \mathbf{x}_{k+1:d} = (\mathbf{z}_{k+1:d} - \mu(\mathbf{z}_{1:k})) \odot \exp(-\sigma(\mathbf{z}_{1:k})) \tag{22}\]

<script src="https://gist.github.com/gebob19/4458074fa1e804ad14e704a4e246c3ec.js"></script>

<h2 id="summary">Summary</h2>

<p>And voilà, the recipe for R-NVP is complete!</p>

<p>To summarize we now know how to compute \(f(\mathbf{x})\), \(\log(\mid\det(\dfrac{df}{d\mathbf{x}})\mid)\), and \(f^{-1}(\mathbf{z})\).</p>

<p>Below is the full jupyter notebook with PyTorch code for model optimization and data generation.</p>

<p><a href="https://github.com/gebob19/introduction_to_normalizing_flows">Jupyter Notebook</a></p>

<p><em>Note:</em> In the notebook the multilayer R-NVP flips the input before a forward/inverse pass for a more expressive model.</p>

<h3 id="optimizing-model">Optimizing Model</h3>

\[\log(p_X(x)) = \log(\mid det(\dfrac{df}{dx}) \mid) + \log(p_Z(f(x)))\]

\[\log(p_X(x)) = \sum_{i=1}^{n} \log(\mid det(\dfrac{dz_i}{dz_{i-1}}) \mid) + \log(p_Z(f(x)))\]

<script src="https://gist.github.com/gebob19/7440c0c0473749f7c3fed67ee3e25962.js"></script>

<h3 id="generating-data-from-model">Generating Data from Model</h3>

\[z \sim p_Z\]

\[x_{gen} = f^{-1}(z)\]

<script src="https://gist.github.com/gebob19/f453a654da8ff5ecd41978b9ce6b9fc8.js"></script>

<h1 id="conclusion">Conclusion</h1>

<p>In summary, we learned how to model a data distribution to a chosen latent-distribution using an invertible function \(f\). We used the change of variables formula to discover that to model our data we must maximize the Jacobian of \(f\) while also constraining \(f\) to our latent-distribution. We then extended this notion to sequentially applying multiple functions \(f_n \circ \dots \circ f_1(x)\). Lastly, we learned about the theory and implementation of the R-NVP flow.</p>

<p>Thanks for reading!</p>

<p>Question? Criticism? Phrase? Advice? Topic you want to be covered? Leave a comment in the section below!</p>

<p>Want more content? Follow me on <a href="https://twitter.com/brennangebotys">Twitter</a>!</p>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="rezende2015variational">Rezende, D. J., &amp; Mohamed, S. (2015). Variational inference with normalizing flows. <i>ArXiv Preprint ArXiv:1505.05770</i>.</span></li>
<li><span id="vaebayes">Kingma, D. P., &amp; Welling, M. (2013). <i>Auto-Encoding Variational Bayes</i>.</span></li>
<li><span id="NIPS2014_5423">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative Adversarial Nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, &amp; K. Q. Weinberger (Eds.), <i>Advances in Neural Information Processing Systems 27</i> (pp. 2672–2680). Curran Associates, Inc. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</span></li>
<li><span id="kingma2018glow">Kingma, D. P., &amp; Dhariwal, P. (2018). Glow: Generative flow with invertible 1x1 convolutions. <i>Advances in Neural Information Processing Systems</i>, 10215–10224.</span></li>
<li><span id="dinh2016density">Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2016). Density estimation using real nvp. <i>ArXiv Preprint ArXiv:1605.08803</i>.</span></li>
<li><span id="dinh2014nice">Dinh, L., Krueger, D., &amp; Bengio, Y. (2014). Nice: Non-linear independent components estimation. <i>ArXiv Preprint ArXiv:1410.8516</i>.</span></li></ol>

<h2 id="more-resources">More Resources</h2>

<ul>
  <li>
    <p>Indepth analysis of more recent flows: <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html</a></p>
  </li>
  <li>
    <p>More flows and their equations: <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">http://akosiorek.github.io/ml/2018/04/03/norm_flows.html</a></p>
  </li>
  <li>
    <p>Tensorflow Normalizing Flow Tutorial: <a href="https://blog.evjang.com/2018/01/nf1.html">https://blog.evjang.com/2018/01/nf1.html</a></p>
  </li>
  <li>
    <p>Video resource on the change of variables formulation: <a href="https://www.youtube.com/watch?v=OeD3RJpeb-w">https://www.youtube.com/watch?v=OeD3RJpeb-w</a></p>
  </li>
</ul>

</article>

  <span class="post-date">
  Written on
  
  July
  17th,
  2019
  by
  
    Brennan Gebotys
  
</span>



  <div class="post-date">Feel free to share!</div>
<div class="sharing-icons">
  <a href="https://twitter.com/intent/tweet?text=Going with the Flow: An Introduction to Normalizing Flows&amp;url=/normalizing-flows/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  <a href="https://www.facebook.com/sharer/sharer.php?u=/normalizing-flows/&amp;title=Going with the Flow: An Introduction to Normalizing Flows" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
</div>



  <div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
      
        
          <li>
            <h3>
              <a href="/natural-gradient/">
                The Natural Gradient without the Tears
                <!--<img src="https://gebob19.github.io/images/">-->
                <!--<small>July 2, 2021</small>-->
              </a>
            </h3>
          </li>
          
        
      
        
          <li>
            <h3>
              <a href="/tfrecords/">
                Video TFRecords: How to Efficiently Load Video Data
                <!--<img src="https://gebob19.github.io/images/">-->
                <!--<small>November 16, 2020</small>-->
              </a>
            </h3>
          </li>
          
        
      
        
          <li>
            <h3>
              <a href="/recursive-generative-models/">
                Generative Models: Recursive Edition
                <!--<img src="https://gebob19.github.io/images/">-->
                <!--<small>July 23, 2020</small>-->
              </a>
            </h3>
          </li>
          
        
      
    
      
        
        
      
        
        
      
        
        
      
    
  </ul>
</div>




      </div>
      <footer class="footer">
  
  
  
    <a href="https://www.github.com/gebob19" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/brennangebotys" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <div class="footer-description"><a href="/">Brennan Gebotys | Machine Learning, Statistics, and All Things Cool by Brennan Gebotys</a></div>
</footer>

    </div>
  </body>
</html>
